{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FAISS+FASTTEXT .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ceB9dQL2hAdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68321a39-2f7f-480e-faba-e2df2aad9867"
      },
      "source": [
        "#@title install\r\n",
        "!pip install fasttext\r\n",
        "!apt install libomp-dev\r\n",
        "!python -m pip install --upgrade faiss faiss-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (53.0.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.19.5)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp-dev is already the newest version (5.0.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n",
            "Requirement already up-to-date: faiss in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already up-to-date: faiss-gpu in /usr/local/lib/python3.6/dist-packages (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from faiss) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "pcx7Q7sHhH3S"
      },
      "source": [
        "#@title imports\r\n",
        "import faiss, fasttext as ft, json, string, numpy as np, pandas as pd,fasttext.util as ftu\r\n",
        "printable = set(string.printable)\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwMjaM24hH0a"
      },
      "source": [
        "#@title constants { form-width: \"20%\" }\r\n",
        "MODEL_PATH = '/content/drive/MyDrive/models/fasttext/cc.en.300.bin'\r\n",
        "SQUAD_TRAINING_DATA_PATH = \"/content/drive/MyDrive/Colab data/squad/train-v2.0.json\"\r\n",
        "FAISS_TOPK = 10"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVfucF6E5SFN"
      },
      "source": [
        "# **FUNCTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-eBR-0jAhWdY"
      },
      "source": [
        "#@title embedding functions: create vector and get distance\r\n",
        "def getSentenceVector(s):\r\n",
        "  return model.get_sentence_vector(s)\r\n",
        "\r\n",
        "def cosine_distance_wordembedding_method(vector_1,vector_2,doEuclidean=False):\r\n",
        "  if doEuclidean:\r\n",
        "    cosine = scipy.spatial.distance.euclidean(vector_1, vector_2)\r\n",
        "  else:\r\n",
        "    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\r\n",
        "  return round( (1-cosine) , 2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TITz73QThc4G",
        "cellView": "form"
      },
      "source": [
        "#@title FAISS functions : create and search\r\n",
        "\r\n",
        "'''\r\n",
        "given a query and index of faiss. this returns top indexes and their similarity scores\r\n",
        "@params:\r\n",
        "query: string -> query \r\n",
        "index: faiss.swigfaiss.IndexIDMap  -> faiss index map which is returned by createFaissIndexing\r\n",
        "'''\r\n",
        "def searchQueryInFaiss(query,index,top_k):\r\n",
        "  D, I = index.search(np.array([getSentenceVector(query)]), k=top_k)\r\n",
        "  similarityScores = D.flatten().tolist()\r\n",
        "  topIndexes = I.flatten().tolist()\r\n",
        "  return similarityScores,topIndexes\r\n",
        "\r\n",
        "def createFaissIndexing_internal(embeddings,embeddingsCounter):\r\n",
        "  # Step 2: Instantiate the index\r\n",
        "  index = faiss.IndexFlatL2(embeddings.shape[1])\r\n",
        "\r\n",
        "  # Step 3: Pass the index to IndexIDMap\r\n",
        "  index = faiss.IndexIDMap(index)\r\n",
        "\r\n",
        "  # Step 4: Add vectors and their IDs\r\n",
        "  index.add_with_ids(embeddings, embeddingsCounter)\r\n",
        "\r\n",
        "  return index\r\n",
        "\r\n",
        "'''\r\n",
        "given a list of text(strings) , this function will create embeddings vector\r\n",
        "@params \r\n",
        "data : list of string -> data list\r\n",
        "@returns\r\n",
        "embeddings: list of vectors -> embeddings vector\r\n",
        "embeddingsCounter: list of ints -> a very dumb list that i am not proud of making. contains 0 to n natural numbers , n being the len of the data\r\n",
        "'''\r\n",
        "def createEmbeddings(data):\r\n",
        "  embeddings = []\r\n",
        "  embeddingsCounter = []\r\n",
        "  for eachQuestionCounter,eachQuestion in enumerate(data):\r\n",
        "    localVec = getSentenceVector(eachQuestion)\r\n",
        "    embeddings.append( localVec )\r\n",
        "    embeddingsCounter.append( eachQuestionCounter )\r\n",
        "    \r\n",
        "    # index.add(localVec)   \r\n",
        "\r\n",
        "  embeddings = np.array(embeddings).astype('float32')\r\n",
        "  embeddingsCounter = np.array(embeddingsCounter)\r\n",
        "  return embeddings,embeddingsCounter\r\n",
        "\r\n",
        "#given a list of texts, this function will create embeddings and create faiss indexing and return the index\r\n",
        "def createFaissIndexing(data):\r\n",
        "  embeddings,embeddingsCounter = createEmbeddings(data)\r\n",
        "  return createFaissIndexing_internal( embeddings,embeddingsCounter  )\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "PTZ2z2ndijOk"
      },
      "source": [
        "#@title normalize_text function { form-width: \"20%\" }\r\n",
        "def normalize_text(s,isLower=True):\r\n",
        "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\r\n",
        "\r\n",
        "  # return white_space_fix(remove_articles(remove_punc(removeSlashTags(lower(s)))))\r\n",
        "  if isLower:\r\n",
        "    return white_space_fix(removeSlashTags(remonveNonPrintables(s.lower())))\r\n",
        "  else:\r\n",
        "    return white_space_fix(removeSlashTags(remonveNonPrintables(s)))\r\n",
        "\r\n",
        "\r\n",
        "def removeSlashTags(text):\r\n",
        "  replace_characters = ['\\r','\\n','\\\\','\\t','/','\\b',\"-\"]\r\n",
        "  for ch in replace_characters:\r\n",
        "    if ch in text:\r\n",
        "      text = text.replace(ch,\" \")\r\n",
        "  text = text.replace(\"__\",\"\")\r\n",
        "  # text = text.replace(\"-\",\"\")\r\n",
        "  return text\r\n",
        "\r\n",
        "def remove_articles(text):\r\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\r\n",
        "    return re.sub(regex, \" \", text)\r\n",
        "\r\n",
        "def white_space_fix(text):\r\n",
        "    return \" \".join(text.split())\r\n",
        "\r\n",
        "def remove_punc(text):\r\n",
        "    exclude = set(string.punctuation)\r\n",
        "    return \"\".join(ch for ch in text if ch not in exclude)\r\n",
        "\r\n",
        "def remonveNonPrintables(text):\r\n",
        "  return \"\".join(filter(lambda c: c in printable, (text) ))\r\n",
        "\r\n",
        "def normalizeTextForList(strList):\r\n",
        "  strListRet = []\r\n",
        "  for i in strList:\r\n",
        "    strListRet.append(normalize_text(i))\r\n",
        "\r\n",
        "  return strListRet"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1LSimxYti8qz"
      },
      "source": [
        "#@title IndexToText\r\n",
        "def IndexToText(topIndexes,allQuestions):\r\n",
        "  retData= []\r\n",
        "  for eachTopIndex in topIndexes:\r\n",
        "    retData.append(allQuestions[eachTopIndex])\r\n",
        "\r\n",
        "  return retData"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "xWGdoGLH3veM"
      },
      "source": [
        "#@title squad data reading function \r\n",
        "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\r\n",
        "                           verbose = 1):\r\n",
        "    \"\"\"\r\n",
        "    input_file_path: path to the squad json file.\r\n",
        "    record_path: path to deepest level in json file default value is\r\n",
        "    ['data','paragraphs','qas','answers']\r\n",
        "    verbose: 0 to suppress it default is 1\r\n",
        "    \"\"\"\r\n",
        "    if verbose:\r\n",
        "        print(\"Reading the json file\")    \r\n",
        "    file = json.loads(open(input_file_path).read())\r\n",
        "    if verbose:\r\n",
        "        print(\"processing...\")\r\n",
        "    # parsing different level's in the json file\r\n",
        "    js = pd.io.json.json_normalize(file , record_path )\r\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\r\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\r\n",
        "    \r\n",
        "    #combining it into single dataframe\r\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\r\n",
        "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\r\n",
        "    m['context'] = idx\r\n",
        "    js['q_idx'] = ndx\r\n",
        "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\r\n",
        "    main['c_id'] = main['context'].factorize()[0]\r\n",
        "    if verbose:\r\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\r\n",
        "        print(\"Done\")\r\n",
        "    return main"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3pCpFls5eW7"
      },
      "source": [
        "# **MAIN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7Tel0E6hHx6",
        "cellView": "form"
      },
      "source": [
        "#@title model loading \r\n",
        "#if not already download then first download. if downloaded then just pass the location path to load the model. I already have the model downlaoded\r\n",
        "\r\n",
        "# ftu.download_model('en', if_exists='ignore')  # English\r\n",
        "model = ft.load_model(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_mxdPqkitus",
        "cellView": "form"
      },
      "source": [
        "#@title reading data\r\n",
        "df = squad_json_to_dataframe_train(SQUAD_TRAINING_DATA_PATH)\r\n",
        "questionsList = df[\"question\"]\r\n",
        "questionsList = normalizeTextForList(questionsList)\r\n",
        "len(questionsList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "udbmOYcD9Tru"
      },
      "source": [
        "#@title creating faiss index\r\n",
        "faissIndex = createFaissIndexing(questionsList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPAi47rPic9K"
      },
      "source": [
        "#@title finding similar questions { form-width: \"20%\" }\r\n",
        "\r\n",
        "#define query \r\n",
        "myIndex = 110\r\n",
        "query = questionsList[myIndex]\r\n",
        "# query = \"any query\"\r\n",
        "\r\n",
        "print(\"Question : \",query)\r\n",
        "#get top indexes and similarity scores\r\n",
        "similarityScores,topIndexes = searchQueryInFaiss(query,faissIndex,FAISS_TOPK)\r\n",
        "#get top indexes' text\r\n",
        "similarTexts = IndexToText(topIndexes,questionsList)\r\n",
        "\r\n",
        "#score -> lower the better. ranked list\r\n",
        "for index,score,text in zip(topIndexes,similarityScores,similarTexts):\r\n",
        "  if index!=myIndex:\r\n",
        "    print(index ,\" -> \", score ,\"->\",text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-INVZAsiB9M"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXGva3VL33Wv"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beff_F_s398V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}